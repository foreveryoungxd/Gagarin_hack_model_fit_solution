{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c0596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from random import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize, download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsNERTagger,\n",
    "    ORG,\n",
    "    Doc,\n",
    "    NewsEmbedding,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import catboost\n",
    "import optuna\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413ce02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = NewsEmbedding()\n",
    "segmenter = Segmenter()\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12480ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(\n",
    "    ['—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ', '–Ω–µ', '–Ω–∞', '–Ω–æ', '–∑–∞', '—Ç–æ', '—Å', '–ª–∏', '–∞', '–≤–æ', '–æ—Ç', '—Å–æ',\n",
    "     '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', '–±—ã', '—á—Ç–æ', '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–∏–∑-–∑–∞', '—Ç–∞–∫–∂–µ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bac94",
   "metadata": {},
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c56c683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = pd.read_csv('mentions.csv', index_col=0)\n",
    "sentiment = pd.read_csv('sentiment.csv', index_col=0)\n",
    "\n",
    "issuers = pd.read_excel('issuers.xlsx', index_col=0)\n",
    "additional_ner_data = pd.read_excel('names and synonyms.xlsx', index_col=0)\n",
    "\n",
    "# pickle\n",
    "\n",
    "with open('sentiment_texts.pickle', 'rb') as f:\n",
    "    sentiment_texts = pickle.loads(f.read())\n",
    "    \n",
    "with open('mentions texts.pickle', 'rb') as f:\n",
    "    mentions_texts = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada4515",
   "metadata": {},
   "source": [
    "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞ —Å–µ—Ç–∞:\n",
    "\n",
    "1)      –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–π. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:\n",
    "a.       **mentions.csv**  - —Å–æ–¥–µ—Ä–∂–∏—Ç id –∫–∞–Ω–∞–ª–∞, id —Å–æ–æ–±—â–µ–Ω–∏—è –∏ id —É–ø–æ–º–∏–Ω–∞–µ–º–æ–π –∫–æ–º–ø–∞–Ω–∏–∏  \n",
    "b.       **mentions_texts.pickle** ‚Äì —Å–æ–¥–µ—Ä–∂–∏—Ç id –∫–∞–Ω–∞–ª–∞, id —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è\n",
    "\n",
    "2)      –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:\n",
    "a.       **sentiment.csv** ‚Äì —Å–æ–¥–µ—Ä–∂–∏—Ç id –∫–∞–Ω–∞–ª–∞, id —Å–æ–æ–±—â–µ–Ω–∏—è, id –∫–æ–º–ø–∞–Ω–∏–∏ –∏ score —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞  \n",
    "b.       **sentiment_texts.pickle** - —Å–æ–¥–µ—Ä–∂–∏—Ç id –∫–∞–Ω–∞–ª–∞, id —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f90183",
   "metadata": {},
   "source": [
    "# –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n",
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ—Ä–∞–±–æ—Ç–∞–µ–º —Å –¥–æ–ø. –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –≤–∞—Ä–∏–∞—Ü–∏—è–º –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e643f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 255 entries, 1 to 274\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   EMITENT_FULL_NAME  255 non-null    object \n",
      " 1   VeryOddCompany     0 non-null      float64\n",
      " 2   BGTicker           103 non-null    object \n",
      " 3   BGTicker.1         166 non-null    object \n",
      " 4   Unnamed: 5         207 non-null    object \n",
      " 5   Unnamed: 6         165 non-null    object \n",
      " 6   Unnamed: 7         103 non-null    object \n",
      " 7   Unnamed: 8         53 non-null     object \n",
      " 8   Unnamed: 9         16 non-null     object \n",
      " 9   Unnamed: 10        5 non-null      object \n",
      " 10  Unnamed: 11        2 non-null      object \n",
      " 11  Unnamed: 12        1 non-null      object \n",
      " 12  Unnamed: 13        1 non-null      object \n",
      " 13  Unnamed: 14        1 non-null      object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 29.9+ KB\n"
     ]
    }
   ],
   "source": [
    "additional_ner_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b207f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_values(row):\n",
    "    return [value for value in row if not pd.isna(value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4f26433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_substrings(text:str, substring:str):\n",
    "    if isinstance(text, str):\n",
    "        if substring in text:\n",
    "            text = text.replace(substring, '')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0a9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_ner_data.BGTicker = additional_ner_data.BGTicker.apply(lambda x: delete_substrings(x, 'RX')) #–ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ RX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72dcdb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_ner_data['all_companies_names_mentions'] = additional_ner_data.apply(combine_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7815d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names_dict = additional_ner_data['all_companies_names_mentions'].to_dict() #–°–æ–∑–¥–∞–¥–∏–º —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ id –∫–æ–º–ø–∞–Ω–∏–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º —Å–ø–∏—Å–æ–∫ –∏–∑ –µ–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f769771",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names_dict = {company_id: list(map(str.lower, company_names)) for company_id, company_names in all_names_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10b82b",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—å, –∏ –ø–æ –∫–ª—é—á—É (issuerid) –º—ã –ø–æ–ª—É—á–∏–º –≤—Å–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏  \n",
    "–í–µ—Ä–Ω–µ–º—Å—è –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º **mentions** –∏ **mentions_texts** –∏ –∑–∞–π–º–µ–º—Å—è –∏—Ö –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2815a54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>messageid</th>\n",
       "      <th>issuerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.191500e+04</td>\n",
       "      <td>21915.000000</td>\n",
       "      <td>21915.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.201366e+09</td>\n",
       "      <td>48830.289710</td>\n",
       "      <td>125.302441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.079644e+08</td>\n",
       "      <td>77917.270287</td>\n",
       "      <td>78.205528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.138795e+09</td>\n",
       "      <td>4837.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.203561e+09</td>\n",
       "      <td>12390.000000</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.219485e+09</td>\n",
       "      <td>48683.500000</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.868097e+09</td>\n",
       "      <td>278484.000000</td>\n",
       "      <td>274.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ChannelID      messageid      issuerid\n",
       "count  2.191500e+04   21915.000000  21915.000000\n",
       "mean   1.201366e+09   48830.289710    125.302441\n",
       "std    1.079644e+08   77917.270287     78.205528\n",
       "min    0.000000e+00       0.000000     -3.000000\n",
       "25%    1.138795e+09    4837.000000     53.000000\n",
       "50%    1.203561e+09   12390.000000    115.000000\n",
       "75%    1.219485e+09   48683.500000    197.000000\n",
       "max    1.868097e+09  278484.000000    274.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.describe() #–ù–∞–±–ª—é–¥–∞–µ–º –≤—ã–±—Ä–æ—Å –≤ –≤–∏–¥–µ issuerid == -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67e064d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>messageid</th>\n",
       "      <th>issuerid</th>\n",
       "      <th>MessageID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.935500e+04</td>\n",
       "      <td>19355.000000</td>\n",
       "      <td>19355.000000</td>\n",
       "      <td>19355.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.200943e+09</td>\n",
       "      <td>52060.347507</td>\n",
       "      <td>124.081839</td>\n",
       "      <td>52060.347507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.781432e+07</td>\n",
       "      <td>81240.604989</td>\n",
       "      <td>78.133251</td>\n",
       "      <td>81240.604989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.001030e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.168461e+09</td>\n",
       "      <td>4943.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4943.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.203561e+09</td>\n",
       "      <td>13114.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>13114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.203561e+09</td>\n",
       "      <td>52838.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>52838.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.565800e+09</td>\n",
       "      <td>278484.000000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>278484.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ChannelID      messageid      issuerid      MessageID\n",
       "count  1.935500e+04   19355.000000  19355.000000   19355.000000\n",
       "mean   1.200943e+09   52060.347507    124.081839   52060.347507\n",
       "std    9.781432e+07   81240.604989     78.133251   81240.604989\n",
       "min    1.001030e+09       5.000000     -2.000000       5.000000\n",
       "25%    1.168461e+09    4943.000000     53.000000    4943.000000\n",
       "50%    1.203561e+09   13114.000000    115.000000   13114.000000\n",
       "75%    1.203561e+09   52838.000000    190.000000   52838.000000\n",
       "max    1.565800e+09  278484.000000    274.000000  278484.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_texts.describe() #–ù–∞–±–ª—é–¥–∞–µ–º –≤—ã–±—Ä–æ—Å –≤ –≤–∏–¥–µ issuerid == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f3cc1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>messageid</th>\n",
       "      <th>issuerid</th>\n",
       "      <th>MessageID</th>\n",
       "      <th>DateAdded</th>\n",
       "      <th>DatePosted</th>\n",
       "      <th>MessageText</th>\n",
       "      <th>IsForward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15771</th>\n",
       "      <td>1203560567</td>\n",
       "      <td>2275</td>\n",
       "      <td>-2</td>\n",
       "      <td>2275</td>\n",
       "      <td>2021-02-06 01:47:00</td>\n",
       "      <td>2017-12-20 07:20:09</td>\n",
       "      <td>PSBR S&amp;P Global Ratings –æ—Ü–µ–Ω–∏–ª–æ –≤ 93 –º–ª—Ä–¥ —Ä—É–±....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ChannelID  messageid  issuerid  MessageID           DateAdded  \\\n",
       "15771  1203560567       2275        -2       2275 2021-02-06 01:47:00   \n",
       "\n",
       "               DatePosted                                        MessageText  \\\n",
       "15771 2017-12-20 07:20:09  PSBR S&P Global Ratings –æ—Ü–µ–Ω–∏–ª–æ –≤ 93 –º–ª—Ä–¥ —Ä—É–±....   \n",
       "\n",
       "       IsForward  \n",
       "15771      False  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_texts[mentions_texts.issuerid <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c8ecf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>messageid</th>\n",
       "      <th>issuerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11217</th>\n",
       "      <td>1280537383</td>\n",
       "      <td>18186</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16881</th>\n",
       "      <td>1203560567</td>\n",
       "      <td>2275</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ChannelID  messageid  issuerid\n",
       "11217  1280537383      18186        -3\n",
       "16881  1203560567       2275        -2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions[mentions.issuerid <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bb0bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_texts = mentions_texts[mentions_texts.issuerid > 0] #–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –≤—ã–±—Ä–æ—Å–∞–º–∏\n",
    "mentions = mentions[mentions.issuerid > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb426e",
   "metadata": {},
   "source": [
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –∑–Ω–∞—á–∏—Ç, –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö **messageid** –∏–º–µ–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ **issuerid**, —Ç.–µ. —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ  \n",
    "–Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤ –æ–¥–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "256e1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_texts = mentions_texts[['messageid', 'issuerid', 'MessageText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749aabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = mentions[['messageid', 'issuerid']].groupby('messageid', as_index=False).agg({'issuerid': list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0ffcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_texts = mentions_texts.drop_duplicates(subset=['MessageText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f16067",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_full_data = pd.merge(mentions, mentions_texts[['messageid', 'MessageText']],\n",
    "                              on='messageid',\n",
    "                              how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "706408e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_eng_words = [] #–°–¥–µ–ª–∞–µ–º —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü–µ –∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–¥–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "for values in all_names_dict.values():\n",
    "    for item in values:\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]{5,}\\b', item)\n",
    "        dict_eng_words.extend(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da06a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_comps_mentions = [] #–ù–∞—á–Ω–µ–º —É–¥–∞–ª—è—Ç—å –∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤ MessageText –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∏—Å–∫–æ–º—ã–º–∏\n",
    "\n",
    "for text in mentions_full_data.MessageText:\n",
    "    latin_words = re.findall(r'\\b[a-zA-Z]{5,}\\b', text) #–Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º/—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ, –¥–ª–∏–Ω–æ–π > 4 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    \n",
    "    latin_comps_mentions.extend(latin_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe847772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for latin_word in latin_comps_mentions:\n",
    "    if latin_word.lower() in dict_eng_words:\n",
    "        latin_comps_mentions.remove(latin_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa66f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_comps_mentions = list(set(latin_comps_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2879a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_tokens = [\n",
    "    '–º–ª—Ä–¥', \n",
    "    '–º–ª–Ω', \n",
    "    '—Ç—Ä–ª–Ω',\n",
    "    '—Ä—É–±',\n",
    "    '–≥–æ–¥–∞',\n",
    "    '–∫–æ–º–ø–∞–Ω–∏–∏', \n",
    "    '–∫–æ–º–ø–∞–Ω–∏—è', \n",
    "    '–∫–æ–º–ø–∞–Ω–∏–π', \n",
    "    '–∞–∫—Ü–∏–∏', \n",
    "    '–∞–∫—Ü–∏–π',\n",
    "    '—Ä—É–±–ª–µ–π',\n",
    "    '–≥–æ–¥',\n",
    "    '–Ω–∞',\n",
    "    '–≥–æ–¥—É',\n",
    "    '–ø–æ',\n",
    "    '—Å—à–∞',\n",
    "    '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã',\n",
    "    '–º—Å—Ñ–æ',\n",
    "    '–Ω–µ—Ñ—Ç—å', \n",
    "    '–Ω–µ—Ñ—Ç–∏',\n",
    "    'ru',\n",
    "    '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤', \n",
    "    '–î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤'\n",
    "    '–ø—Ä–∏',\n",
    "    '—Å–∏—Å—Ç–µ–º–∞',\n",
    "    '30–º—Å–∫', \n",
    "    '00–º—Å–∫',\n",
    "    '—Å–æ–≤–µ—Ç', \n",
    "    '–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤', ''\n",
    "    'ak47pfl',\n",
    "    '–µ—Å–ª–∏',\n",
    "    '–≤–æ–ø—Ä–æc',\n",
    "    '–Ω–æ–≤–æ—Å—Ç–∏',\n",
    "    '–ø–æ–¥—Ä–æ–±–Ω–µ–µ',\n",
    "    '—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö',\n",
    "    '—Ä–∞–º–∫–∞—Ö',\n",
    "    'ipo',\n",
    "    '–≤–∑–≥–ª—è–¥',\n",
    "    '—Ä—Ñ',\n",
    "    '—Ä–æ—Å—Å–∏–∏', \n",
    "    '—Ä–æ—Å—Å–∏—è', \n",
    "    '—Ä–æ—Å—Å–∏–µ',\n",
    "    '–Ω–µ',\n",
    "    '–¥–Ω—è',\n",
    "    '–¥–∏–≤–∏–¥–µ–Ω–¥—ã',\n",
    "    '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å', \n",
    "    '–æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å',\n",
    "    '–±–∏—Ä–∂–∞',\n",
    "    '–±–∏—Ä–∂–µ',\n",
    "    '–±–∏—Ä–∂–∏',\n",
    "    '–∫–≤',\n",
    "    '–º—Å–∫',\n",
    "    '–≥–≥',\n",
    "    '—Ñ–Ω',\n",
    "    'upd'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "845816c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_company_name_natasha = [\n",
    "    '–ª–æ–Ω–≥', \n",
    "    '–º–∏—Ä–∞', \n",
    "    '–≥–æ—Ä—è—á—É—é', \n",
    "    '–∫–∞–∑–Ω–∞—á–µ–π—Å—Ç–≤–æ', \n",
    "    '—Ä–∞—Å–ø–∞–¥—Å–∫–æ–π', \n",
    "    '–¥–µ—Ç—Å–∫–æ–≥–æ', \n",
    "    '–≤–µ–¥–æ–º–æ—Å—Ç–∏', \n",
    "    '–∞–∫—Ü–∏–æ–Ω–µ—Ä—ã', \n",
    "    '—à–æ—Ä—Ç', \n",
    "    '–¥–µ—Å—è—Ç–∫—É', \n",
    "    '–∞–∫—Ü–∏—è'\n",
    "    '—Ü–±',\n",
    "    '–º–æ—Å–±–∏—Ä–∂–∏', \n",
    "    '–º–æ—Å–±–∏—Ä–∂–∞',\n",
    "    '–±–∞–Ω–∫–∞',\n",
    "    '—Å–¥',\n",
    "    '–∫–æ–º–ø–∞–Ω–∏—è',\n",
    "    '—Å–æ–≤–µ—Ç',\n",
    "    '—Å–∏—Å—Ç–µ–º–∞',\n",
    "    '–ø—Ä–∞–π–º',\n",
    "    '–∞–æ',\n",
    "    '—Ä–∞–æ',\n",
    "    '–º–æ—Å–∫–æ–≤—Å–∫–æ–π',\n",
    "    '—Ç–∞—Å—Å',\n",
    "    '—Ñ—Ä—Å',\n",
    "    '–º–∏–Ω—Ñ–∏–Ω',\n",
    "    '–≥—Ä—É–ø–ø—ã',\n",
    "    '–±–∏—Ä–∂–∞',\n",
    "    '–±–∏—Ä–∂–µ',\n",
    "    '–±–∏—Ä–∂–∏',\n",
    "    'en',\n",
    "    '—Ñ–∞—Å',\n",
    "    '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å',\n",
    "    '–∑–∞',\n",
    "    '–≤—ã—Ä—É—á–∫–∞',\n",
    "    '–Ω–µ',\n",
    "    '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç',\n",
    "    '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', \n",
    "    '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π',\n",
    "    '—Å–º–∏',\n",
    "    '—Ñ–ª—ç—Ç',\n",
    "    '—Ñ–æ–Ω–¥',\n",
    "    '—Ç–æ–ø',\n",
    "    '–Ω–æ–≤–æ—Å—Ç–∏',\n",
    "    '–±–æ–∫–æ–≤–∏–∫',\n",
    "    '–º–∏–Ω—Ñ–∏–Ω–∞',\n",
    "    '—Å–∏—Å—Ç–µ–º—ã',\n",
    "    '—Ü–µ–Ω—Ç—Ä',\n",
    "    '—Ñ–Ω',\n",
    "    '—Å–µ–≥–æ–¥–Ω—è',\n",
    "    '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª',\n",
    "    '–ø—Ä–æ–≥–Ω–æ–∑',\n",
    "    '—Å–∏–≥–Ω–∞–ª–æ–≤',\n",
    "    '–æ—Ç',\n",
    "    '—Ä–∞–∑–º–µ—Ä–µ',\n",
    "    '—Ä–∏–∞',\n",
    "    '–æ–±—ä–µ–º',\n",
    "    '–∞—É—Ç—Å–∞–π–¥–µ—Ä—ã',\n",
    "    '–æ—Ç—á–µ—Ç',\n",
    "    '–ø—Ä–∞–≤–ª–µ–Ω–∏—è',\n",
    "    '–º–æ—Å–∫–æ–≤—Å–∫–∏–π',\n",
    "    '—Å–ª—É–∂–±–∞',\n",
    "    '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π',\n",
    "    '–ø—Ä–∏–∫–∞–∑',\n",
    "    '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å–∞',\n",
    "    '—Ä–∞—Å–ø–∞–¥—Å–∫–æ–π',\n",
    "    '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π',\n",
    "    '–¥–æ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e56f1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_text_preprocess(text): #–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "    remove_punctuation = '!\"#$%&\\'*+,./:;<=>?@[\\\\]^_`{|}~``üá∑üá∫'')('\n",
    "    text = re.sub(r'\\s+', ' ', text) #–ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª\n",
    "    text = re.sub(r'(?<=[^\\w\\d])-|-(?=[^\\w\\d])|[^\\w\\d\\s-]', '', text) #–£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥–µ—Ñ–∏—Å–æ–≤\n",
    "    text = re.sub(r'\\+\\d{1,2}\\s\\(\\d{3}\\)\\s\\d{3}-\\d{2}-\\d{2}', '', text) #—É–¥–∞–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞\n",
    "    text = re.sub(r'https?://\\S+', '', text) #—É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏\n",
    "    text = re.sub(r'\\s+', ' ', text) #–µ—â–µ —Ä–∞–∑ —É–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã, –µ—Å–ª–∏ —Ç–∞–∫–æ–≤—ã–µ –æ—Å—Ç–∞–ª–∏—Å—å\n",
    "    text = re.sub('‚Ä¢', '', text)\n",
    "    text = re.sub(\"''\", '', text)\n",
    "    text = re.sub(r'[¬´¬ª]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text) #—É–¥–∞–ª—è–µ–º –≤—Å–µ —Ü–∏—Ñ—Ä—ã\n",
    "    text = re.sub(r'\\b\\w\\b', '', text) #—É–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\n",
    "\n",
    "    \n",
    "    tokens = word_tokenize(text) #–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "    filtered_tokens = [token for token in tokens if token not in remove_punctuation and token not in russian_stopwords and token.lower() not in latin_comps_mentions and token.lower() not in most_common_tokens]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a89d18e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 49s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions_full_data['MessageText'] = mentions_full_data['MessageText'].apply(message_text_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7bf21-8655-40f6-9592-64eb7c1c3234",
   "metadata": {},
   "source": [
    "–ë–ª–æ–∫ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º—ã—Ö —Å–ª–æ–≤, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –∏–∑–±–∞–≤–∏—Ç—å—Å—è. –î–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ —Å–ø–∏—Å–æ–∫ most_common_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "174e3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_text_corpus = []\n",
    "for value_list in mentions_full_data.MessageText:\n",
    "    tokens = word_tokenize(value_list)\n",
    "    mes_text_corpus.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75a6b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mes_text_corpus)\n",
    "vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b666f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = X.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6eb24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq = {token: count for token, count in zip(vocabulary, token_counts.tolist()[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68f7160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "top_tokens = sorted_tokens[:200]  # –ü–µ—Ä–≤—ã–µ 10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947d2c8",
   "metadata": {},
   "source": [
    "# –ò–¥–µ—è:  \n",
    "‚Ä¢ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ natasha –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞: –∏–∑–≤–ª–µ–∫–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏, –æ—Ç –∫–∞–∂–¥–æ–π –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å–∏–º–æ–≤–æ–ª–æ–≤  \n",
    "‚Ä¢ –Ω–∞ —Å–ª–æ–≤–∞—Ä–µ –∏–∑ –≤—Å–µ—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π –∏ –æ–±—â–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞, –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ natasha'–µ–π –æ–±—É—á–∞–µ–º tf-idf  \n",
    "‚Ä¢ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è id —É–ø–æ–º–∏–Ω–∞–µ–º–æ–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ –±–µ—Ä–µ–º —Ç–æ, —á—Ç–æ –∏–∑–≤–ª–µ–∫–ª–∞ natasha, –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–¥–æ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞. id –∫–æ–º–ø–∞–Ω–∏–∏ —Å —Ç–æ–ø-1 —Å–∫–æ—Ä–æ–º –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Ä–∞–∑—É, –∞ –µ—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ–ø-1 –∏ —Ç–æ–ø-2 –Ω–µ –±–æ–ª–µ–µ 0.05, —Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º id –æ–±–µ–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "611cea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_natasha_mentions(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger) \n",
    "    orgs = []\n",
    "    for item in doc.spans:\n",
    "        if len(orgs) >= 3: #–ï—Å–ª–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–æ 3 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ\n",
    "            break\n",
    "        if item.type == 'ORG' and item.text.lower() not in not_company_name_natasha:\n",
    "            orgs.append(item.text[:10])\n",
    "    return orgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39e9d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 2s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions_full_data['natasha_mentions'] = mentions_full_data['MessageText'].apply(fill_natasha_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b58945c-453c-4d2f-afc4-2bea61acf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_natasha_corpus = [] #–∏—â–µ–º —Ç–æ–ø —Å–∞–º—ã—Ö –≤—Å—Ç—Ä–µ—á–∞–µ–º—ã—Ö —Å–ª–æ–≤ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ä–∞–±–æ—Ç—ã natasha, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏–µ–π, –∫–ª–∞–¥–µ–º –≤ not_company_name_natasha\n",
    "for value_list in mentions_full_data.natasha_mentions:\n",
    "    text = ' '.join(value_list)  #–û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
    "    tokens = word_tokenize(text)  #–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É\n",
    "    mes_natasha_corpus.extend(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43fe9ed5-1b7f-4502-bb0e-b4ce0acb740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mes_natasha_corpus)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "token_counts = X.sum(axis=0)\n",
    "token_freq = {token: count for token, count in zip(vocabulary, token_counts.tolist()[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "171187b0-f59c-4f3a-9ae4-7e996599b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "top_tokens = sorted_tokens[:400]  # –ü–µ—Ä–≤—ã–µ —Ç–æ–ø __ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef11fe-46d6-4fea-a62f-719d994d0ad2",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a38e25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = []\n",
    "for value_list in all_names_dict.values(): #–ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º –≤ —Å–ª–æ–≤–∞—Ä–µ all_names_dict\n",
    "    text = ' '.join(value_list) #–û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
    "    tokens = word_tokenize(text) #–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É\n",
    "    corpus_tokens.extend(tokens) #–î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä–ø—É—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9705726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation = '!\"#$%&\\'*+,./:;<=>?@[\\\\]''^_`{|}~``üá∑üá∫'')('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae52d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = [word for word in corpus_tokens if word.lower() not in remove_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a6ab0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = [token.replace(\"'\", \"\").replace('\"', '') for token in corpus_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8035d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens_natasha = []\n",
    "for mention_list in mentions_full_data['natasha_mentions']:\n",
    "    text = ' '.join(mention_list) #–û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
    "    tokens = word_tokenize(text) #–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É\n",
    "    corpus_tokens_natasha.extend(tokens) #–î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä–ø—É—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f08576b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_tokens_natasha = [word for word in corpus_tokens_natasha if word.lower() not in remove_punctuation]\n",
    "corpus_tokens_natasha = [token.replace(\"'\", \"\").replace('\"', '') for token in corpus_tokens_natasha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "751f5687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=1087)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1087)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=1087)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf = TfidfVectorizer(max_features=1087) \n",
    "model_tfidf.fit_transform(corpus_tokens)\n",
    "model_tfidf.fit(corpus_tokens_natasha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f356ae1f-0482-40f8-9e51-772d43d13609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_companies(model_tfidf, mentions, all_names_dict): #—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    if len(mentions) == 0: #–¥–ª—è —Å–ª—É—á–∞—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "        return 0\n",
    "    natasha_vector = model_tfidf.transform([' '.join(mentions)]) #–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è Natasha –∏ –∫–æ–º–ø–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–∞ TF-IDF\n",
    "    company_vectors = model_tfidf.transform([' '.join(company_names) for company_names in all_names_dict.values()])\n",
    "    \n",
    "    similarities = cosine_similarity(natasha_vector, company_vectors) #–ù–∞—Ö–æ–¥–∏–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ –∏ –∫–æ–º–ø–∞–Ω–∏—è–º–∏\n",
    "\n",
    "    best_company_id = similarities.argsort(axis=1)[:, -1] + 1  #–∫–æ–º–ø–∞–Ω–∏—è —Å –Ω–∞–∏–≤—ã—Å—à–∏–º —Å–∫–æ—Ä–æ–º\n",
    "    second_best_company_id = similarities.argsort(axis=1)[:, -2] + 1  #–∫–æ–º–ø–∞–Ω–∏—è —Å–æ –≤—Ç–æ—Ä—ã–º –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ —Å–∫–æ—Ä–æ–º\n",
    "\n",
    "    if similarities.max(axis=1)[0] - np.partition(similarities, -2, axis=1)[:, -2][0] > 0.05: #—É—Å–ª–æ–≤–∏–µ —Ä–∞–∑–Ω–∏—Ü—ã —Ç–æ–ø-1 –∏ —Ç–æ–ø-2 —Å–∫–æ—Ä–∞\n",
    "        return best_company_id\n",
    "    else:\n",
    "        return [best_company_id[0], second_best_company_id[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d999923-4164-4a1b-89cd-ff9d1f46aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions_full_data['predict'] = mentions_full_data['natasha_mentions'].apply(lambda x: find_best_companies(model_tfidf, x, all_names_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03ff8db3-3941-411f-aac6-dbecdc352fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_companies(model_tfidf, mentions_full_data.natasha_mentions[0], all_names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94558440-3738-4924-9346-51222ab80207",
   "metadata": {},
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0717bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts = sentiment_texts[['issuerid', 'SentimentScore', 'MessageText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6d9e278-3fcc-4153-b5bd-ecd40e11fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 5, 3, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_texts.SentimentScore.unique() #—É–¥–∞–ª–∏–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –≥–¥–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3e35e3c-9253-4565-a91a-eb958ead32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts = sentiment_texts[sentiment_texts.SentimentScore != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94631847-dc9e-476f-a194-c4d09ac00780",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts = sentiment_texts.groupby('MessageText', as_index=False).agg({'issuerid': list, 'SentimentScore': list}) #–ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º —Ç–µ–∫—Å—Ç–∞–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b7669766-3a28-4efb-b24d-0154ddc7c98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 45.5 s\n",
      "Wall time: 45.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentiment_texts.MessageText = sentiment_texts.MessageText.apply(message_text_preprocess) #–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a646e25a-aab4-405f-853c-79641f5942f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 47s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentiment_texts['natasha_mentions'] = sentiment_texts['MessageText'].apply(fill_natasha_mentions) #–í—ã—è–≤–ª—è–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "48b68af9-ddae-4302-8d24-288a5f9e12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts['predict_id'] = sentiment_texts['natasha_mentions'].apply(lambda x: find_best_companies(model_tfidf, x, all_names_dict)) #–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bfd958ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MessageText</th>\n",
       "      <th>issuerid</th>\n",
       "      <th>SentimentScore</th>\n",
       "      <th>natasha_mentions</th>\n",
       "      <th>predict_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–≤–≤–æ–¥—è—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –±–∞–Ω–∫–æ–≤ –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç...</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>[–ú–æ—Å–∫–æ–≤—Å–∫–∏–π, –í–ü–ö, –¢–ê–°–° TCSG ]</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–≤–≤–æ–¥—è—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –±–∞–Ω–∫–æ–≤ –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç...</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>[–ú–æ—Å–∫–æ–≤—Å–∫–∏–π, –í–ü–ö, –¢–ê–°–° TCSG ]</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ê–ö–†–û–ù –ë–£–î–ï–¢ –û–°–ü–ê–†–ò–í–ê–¢–¨ –†–ï–®–ï–ù–ò–ï –ü–û–õ–¨–®–ò –ê–ö–¶–ò–Ø–ú A...</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>[–ê–ö–†–û–ù –ë–£–î–ï, –ú–ï–ñ–î–£–ù–ê–†–û–î, –ù–ï–ó–ê–ö–û–ù–ù–´–ú]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ê–õ–†–û–°–ê –≤—ã–ø–ª–∞—Ç–∏—Ç—å –∏—Ç–æ–≥–∞–º –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—É–≥–æ–¥–∏—è —Å–∏—Ç—É...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[–ê–õ–†–û–°–ê]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7033</th>\n",
       "      <td>–∫–æ–º–ø–∞–Ω–∏—é –ú–µ—á–µ–ª —ç—Ñ—Ñ–µ–∫—Ç –æ—Ç–º–µ–Ω—ã —ç–∫—Å–ø–æ—Ä—Ç–Ω—ã—Ö –ø–æ—à–ª–∏–Ω...</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>[–ú–µ—á–µ–ª, –ú–µ—á–µ–ª–∞, –ú–µ—á–µ–ª]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7034</th>\n",
       "      <td>–∫–æ–º–ø–∞–Ω–∏—é –ú–µ—á–µ–ª - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–≤–æ—Ä–æ—Ç –î–º–∏—Ç—Ä–∏–π...</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>[–ú–µ—á–µ–ª, –ú–µ—á–µ–ª, –ú–µ—á–µ–ª]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7035</th>\n",
       "      <td>–ú–µ—á–µ–ª –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º —Ä–æ—Å—Ç–∞ —Å–≤—ã—à–µ –∫–æ–Ω—Ü–∞ –∏—Ç–æ–≥–∞–º –¥–µ–∫...</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>[–ú–µ—á–µ–ª, –ú–µ—á–µ–ª, –ú–µ—á–µ–ª–∞]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7036</th>\n",
       "      <td>–ú–µ—á–µ–ª –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –ë–ö–° –ú—ã –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—á–∏—Ç–∞...</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>[–ú–µ—á–µ–ª, –ë–ö–°, –ú–µ—á–µ–ª]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>–ê–§–ö –æ–±–µ—â–∞–µ—Ç –¥–∏–≤—ã –í–µ—Ä–∏–º –û—Å–Ω–æ–≤–∞—Ç–µ–ª—å AFKS –í–ª–∞–¥–∏–º–∏...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>[–ê–§–ö, AFKS, –ê–§–ö –§–∏–Ω–∞–Ω—Å]</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28774 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            MessageText issuerid  \\\n",
       "0                                                            235   \n",
       "1     –≤–≤–æ–¥—è—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –±–∞–Ω–∫–æ–≤ –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç...      100   \n",
       "1     –≤–≤–æ–¥—è—Ç —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –±–∞–Ω–∫–æ–≤ –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç...      100   \n",
       "2     –ê–ö–†–û–ù –ë–£–î–ï–¢ –û–°–ü–ê–†–ò–í–ê–¢–¨ –†–ï–®–ï–ù–ò–ï –ü–û–õ–¨–®–ò –ê–ö–¶–ò–Ø–ú A...       24   \n",
       "3     –ê–õ–†–û–°–ê –≤—ã–ø–ª–∞—Ç–∏—Ç—å –∏—Ç–æ–≥–∞–º –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—É–≥–æ–¥–∏—è —Å–∏—Ç—É...        4   \n",
       "...                                                 ...      ...   \n",
       "7033  –∫–æ–º–ø–∞–Ω–∏—é –ú–µ—á–µ–ª —ç—Ñ—Ñ–µ–∫—Ç –æ—Ç–º–µ–Ω—ã —ç–∫—Å–ø–æ—Ä—Ç–Ω—ã—Ö –ø–æ—à–ª–∏–Ω...       99   \n",
       "7034  –∫–æ–º–ø–∞–Ω–∏—é –ú–µ—á–µ–ª - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–≤–æ—Ä–æ—Ç –î–º–∏—Ç—Ä–∏–π...       99   \n",
       "7035  –ú–µ—á–µ–ª –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º —Ä–æ—Å—Ç–∞ —Å–≤—ã—à–µ –∫–æ–Ω—Ü–∞ –∏—Ç–æ–≥–∞–º –¥–µ–∫...       99   \n",
       "7036  –ú–µ—á–µ–ª –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –ë–ö–° –ú—ã –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—á–∏—Ç–∞...       99   \n",
       "7037  –ê–§–ö –æ–±–µ—â–∞–µ—Ç –¥–∏–≤—ã –í–µ—Ä–∏–º –û—Å–Ω–æ–≤–∞—Ç–µ–ª—å AFKS –í–ª–∞–¥–∏–º–∏...       26   \n",
       "\n",
       "     SentimentScore                      natasha_mentions predict_id  \n",
       "0                 4                                    []          0  \n",
       "1                 2         [–ú–æ—Å–∫–æ–≤—Å–∫–∏–π, –í–ü–ö, –¢–ê–°–° TCSG ]        207  \n",
       "1                 2         [–ú–æ—Å–∫–æ–≤—Å–∫–∏–π, –í–ü–ö, –¢–ê–°–° TCSG ]        137  \n",
       "2                 2  [–ê–ö–†–û–ù –ë–£–î–ï, –ú–ï–ñ–î–£–ù–ê–†–û–î, –ù–ï–ó–ê–ö–û–ù–ù–´–ú]         24  \n",
       "3                 4                              [–ê–õ–†–û–°–ê]         27  \n",
       "...             ...                                   ...        ...  \n",
       "7033              4                [–ú–µ—á–µ–ª, –ú–µ—á–µ–ª–∞, –ú–µ—á–µ–ª]         99  \n",
       "7034              5                 [–ú–µ—á–µ–ª, –ú–µ—á–µ–ª, –ú–µ—á–µ–ª]         99  \n",
       "7035              4                [–ú–µ—á–µ–ª, –ú–µ—á–µ–ª, –ú–µ—á–µ–ª–∞]         99  \n",
       "7036              4                   [–ú–µ—á–µ–ª, –ë–ö–°, –ú–µ—á–µ–ª]         99  \n",
       "7037              4               [–ê–§–ö, AFKS, –ê–§–ö –§–∏–Ω–∞–Ω—Å]         26  \n",
       "\n",
       "[28774 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb2f9658-c749-476e-a2cb-6bbfa0b964c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts_ = sentiment_texts.explode('issuerid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51728835-4ba2-4272-b96b-04362792698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts_ = sentiment_texts_.explode('predict_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "11dd9ba5-b2be-451f-bef2-f65fbdbcd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts_ = sentiment_texts_.explode('SentimentScore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a8c04-7a6c-4fb0-9a00-bcff8e076aa8",
   "metadata": {},
   "source": [
    "# –ë–µ–π–∑–ª–∞–π–Ω - tf-idf + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db372e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(sentiment_texts_, test_size=0.2, random_state=42)\n",
    "test, valid = train_test_split(test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "97dfd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = model_tfidf.fit_transform(train['MessageText'].values)\n",
    "test_tfidf = model_tfidf.transform(test['MessageText'].values)\n",
    "valid_tfidf = model_tfidf.transform(valid['MessageText'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cbd79250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(train_tfidf, train['SentimentScore'].astype(int).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62fd0c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 118 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7994438651372958"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = clf.predict(test_tfidf)\n",
    "accuracy_score(predictions, test['SentimentScore'].astype(int).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3b0ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_clf.joblib']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, 'RF_clf.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb20fa6",
   "metadata": {},
   "source": [
    "# Catboost (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "649428eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = catboost.Pool(train_tfidf, label=train['SentimentScore'].values)\n",
    "test_pool = catboost.Pool(test_tfidf, label=test['SentimentScore'].values)\n",
    "valid_pool = catboost.Pool(valid_tfidf, label=valid['SentimentScore'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8152d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5733959\ttest: 0.5672576\tbest: 0.5672576 (0)\ttotal: 225ms\tremaining: 5m 36s\n",
      "100:\tlearn: 0.7269212\ttest: 0.7278415\tbest: 0.7292318 (95)\ttotal: 7.57s\tremaining: 1m 44s\n",
      "200:\tlearn: 0.7651505\ttest: 0.7518248\tbest: 0.7542579 (195)\ttotal: 14.8s\tremaining: 1m 35s\n",
      "300:\tlearn: 0.7864373\ttest: 0.7643379\tbest: 0.7664234 (255)\ttotal: 22.2s\tremaining: 1m 28s\n",
      "400:\tlearn: 0.7992962\ttest: 0.7737226\tbest: 0.7747654 (391)\ttotal: 29.7s\tremaining: 1m 21s\n",
      "500:\tlearn: 0.8079847\ttest: 0.7803267\tbest: 0.7806743 (495)\ttotal: 37s\tremaining: 1m 13s\n",
      "600:\tlearn: 0.8157609\ttest: 0.7817171\tbest: 0.7834550 (547)\ttotal: 44.3s\tremaining: 1m 6s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7834549878\n",
      "bestIteration = 547\n",
      "\n",
      "Shrink model to first 548 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1a1c010bc90>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_catboost = catboost.CatBoostClassifier(random_state=42, iterations=1500, learning_rate=0.0999, \n",
    "                                             depth=5, l2_leaf_reg=0.11, eval_metric='Accuracy', colsample_bylevel=0.1308)\n",
    "model_catboost.fit(train_pool,\n",
    "                  eval_set = test_pool,\n",
    "                  verbose=100,\n",
    "                  early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c7aa6dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7748436414176512"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid['SentimentScore'].astype(int).values, model_catboost.predict(valid_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d4a53-08d3-4efa-a74d-4dd2bee4edd1",
   "metadata": {},
   "source": [
    "# –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ—Ç—é–Ω–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã catboost —Å –ø–æ–º–æ—â—å—é optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ac6c3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "\n",
    "    params = {\n",
    "         'iterations': trial.suggest_int('iterations', 500, 2000, step=200),\n",
    "         'depth': trial.suggest_int('depth', 3, 6),\n",
    "         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2 , step=0.01),\n",
    "         'auto_class_weights': 'SqrtBalanced',\n",
    "         'eval_metric': \"Accuracy\",\n",
    "         'loss_function': 'MultiClass',\n",
    "         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 1,log=True),\n",
    "         'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "         'random_seed': 42\n",
    "    }\n",
    "\n",
    "\n",
    "    clf_catboost = catboost.CatBoostClassifier(**params)\n",
    "    clf_catboost.fit(train_pool,\n",
    "                      eval_set = test_pool, plot=False, verbose=100,\n",
    "                    early_stopping_rounds=100)\n",
    " \n",
    "    return accuracy_score(valid['SentimentScore'].astype(int).values, clf_catboost.predict(valid_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "108c5c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-14 05:52:15,539] A new study created in memory with name: catboost-seed42\n"
     ]
    }
   ],
   "source": [
    "study_catboost = optuna.create_study(study_name='catboost-seed42',\n",
    "                                direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bc4c2072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa3c1b48ea477d80fa44ee26e878f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5001306\ttest: 0.4783352\tbest: 0.4783352 (0)\ttotal: 215ms\tremaining: 3m 56s\n",
      "100:\tlearn: 0.5614815\ttest: 0.5495169\tbest: 0.5502590 (98)\ttotal: 24.6s\tremaining: 4m 3s\n",
      "200:\tlearn: 0.5983177\ttest: 0.5872105\tbest: 0.5872105 (200)\ttotal: 49s\tremaining: 3m 39s\n",
      "300:\tlearn: 0.6243811\ttest: 0.6201178\tbest: 0.6206276 (298)\ttotal: 1m 13s\tremaining: 3m 16s\n",
      "400:\tlearn: 0.6499665\ttest: 0.6422530\tbest: 0.6425706 (364)\ttotal: 1m 40s\tremaining: 2m 55s\n",
      "500:\tlearn: 0.6543080\ttest: 0.6439926\tbest: 0.6445653 (491)\ttotal: 2m 8s\tremaining: 2m 33s\n",
      "600:\tlearn: 0.6647748\ttest: 0.6520296\tbest: 0.6523549 (595)\ttotal: 2m 33s\tremaining: 2m 7s\n",
      "700:\tlearn: 0.6724939\ttest: 0.6579511\tbest: 0.6579511 (684)\ttotal: 2m 59s\tremaining: 1m 42s\n",
      "800:\tlearn: 0.6784593\ttest: 0.6644937\tbest: 0.6645717 (760)\ttotal: 3m 24s\tremaining: 1m 16s\n",
      "900:\tlearn: 0.6883145\ttest: 0.6778857\tbest: 0.6781330 (892)\ttotal: 3m 48s\tremaining: 50.5s\n",
      "1000:\tlearn: 0.6951881\ttest: 0.6837259\tbest: 0.6837259 (998)\ttotal: 4m 14s\tremaining: 25.1s\n",
      "1099:\tlearn: 0.6980045\ttest: 0.6884706\tbest: 0.6884706 (1091)\ttotal: 4m 38s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.6884706374\n",
      "bestIteration = 1091\n",
      "\n",
      "Shrink model to first 1092 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5076179\ttest: 0.4922546\tbest: 0.4922546 (0)\ttotal: 147ms\tremaining: 2m 41s\n",
      "100:\tlearn: 0.7252107\ttest: 0.7028519\tbest: 0.7040753 (96)\ttotal: 23.8s\tremaining: 3m 55s\n",
      "200:\tlearn: 0.7719510\ttest: 0.7296778\tbest: 0.7306538 (199)\ttotal: 45.1s\tremaining: 3m 21s\n",
      "300:\tlearn: 0.7970614\ttest: 0.7391236\tbest: 0.7391236 (296)\ttotal: 1m 6s\tremaining: 2m 56s\n",
      "400:\tlearn: 0.8098321\ttest: 0.7408820\tbest: 0.7470831 (367)\ttotal: 1m 27s\tremaining: 2m 33s\n",
      "500:\tlearn: 0.8215744\ttest: 0.7502373\tbest: 0.7511855 (494)\ttotal: 1m 49s\tremaining: 2m 10s\n",
      "600:\tlearn: 0.8299044\ttest: 0.7525435\tbest: 0.7539787 (560)\ttotal: 2m 10s\tremaining: 1m 48s\n",
      "700:\tlearn: 0.8362632\ttest: 0.7569006\tbest: 0.7578272 (668)\ttotal: 2m 32s\tremaining: 1m 26s\n",
      "800:\tlearn: 0.8402210\ttest: 0.7556851\tbest: 0.7584477 (746)\ttotal: 2m 53s\tremaining: 1m 4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7584477436\n",
      "bestIteration = 746\n",
      "\n",
      "Shrink model to first 747 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4993854\ttest: 0.4793577\tbest: 0.4793577 (0)\ttotal: 68.5ms\tremaining: 2m 9s\n",
      "100:\tlearn: 0.6419032\ttest: 0.6377060\tbest: 0.6377060 (100)\ttotal: 6.81s\tremaining: 2m 1s\n",
      "200:\tlearn: 0.6821897\ttest: 0.6708889\tbest: 0.6708889 (200)\ttotal: 13.4s\tremaining: 1m 53s\n",
      "300:\tlearn: 0.7034279\ttest: 0.6927091\tbest: 0.6950227 (283)\ttotal: 20.8s\tremaining: 1m 50s\n",
      "400:\tlearn: 0.7233298\ttest: 0.7045264\tbest: 0.7045264 (400)\ttotal: 27.4s\tremaining: 1m 42s\n",
      "500:\tlearn: 0.7370714\ttest: 0.7114952\tbest: 0.7117426 (497)\ttotal: 33.7s\tremaining: 1m 34s\n",
      "600:\tlearn: 0.7484052\ttest: 0.7185641\tbest: 0.7196543 (594)\ttotal: 39.8s\tremaining: 1m 26s\n",
      "700:\tlearn: 0.7591119\ttest: 0.7268940\tbest: 0.7271414 (692)\ttotal: 46s\tremaining: 1m 18s\n",
      "800:\tlearn: 0.7687927\ttest: 0.7292860\tbest: 0.7301911 (794)\ttotal: 52.6s\tremaining: 1m 12s\n",
      "900:\tlearn: 0.7759285\ttest: 0.7316013\tbest: 0.7316793 (860)\ttotal: 59.6s\tremaining: 1m 6s\n",
      "1000:\tlearn: 0.7820000\ttest: 0.7346640\tbest: 0.7346856 (983)\ttotal: 1m 6s\tremaining: 60s\n",
      "1100:\tlearn: 0.7871242\ttest: 0.7358229\tbest: 0.7363177 (1094)\ttotal: 1m 13s\tremaining: 53s\n",
      "1200:\tlearn: 0.7906674\ttest: 0.7346060\tbest: 0.7373222 (1123)\ttotal: 1m 19s\tremaining: 46.2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7373222334\n",
      "bestIteration = 1123\n",
      "\n",
      "Shrink model to first 1124 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4844840\ttest: 0.4715607\tbest: 0.4715607 (0)\ttotal: 85.4ms\tremaining: 1m 50s\n",
      "100:\tlearn: 0.6824615\ttest: 0.6794323\tbest: 0.6794323 (100)\ttotal: 16.1s\tremaining: 3m 10s\n",
      "200:\tlearn: 0.7298625\ttest: 0.7097011\tbest: 0.7124744 (194)\ttotal: 33.4s\tremaining: 3m 2s\n",
      "300:\tlearn: 0.7569110\ttest: 0.7242154\tbest: 0.7242319 (267)\ttotal: 51.2s\tremaining: 2m 49s\n",
      "400:\tlearn: 0.7756952\ttest: 0.7265394\tbest: 0.7322430 (354)\ttotal: 1m 8s\tremaining: 2m 33s\n",
      "500:\tlearn: 0.7878435\ttest: 0.7354554\tbest: 0.7365728 (428)\ttotal: 1m 26s\tremaining: 2m 17s\n",
      "600:\tlearn: 0.7994708\ttest: 0.7385537\tbest: 0.7438625 (574)\ttotal: 1m 43s\tremaining: 2m\n",
      "700:\tlearn: 0.8091197\ttest: 0.7479283\tbest: 0.7480842 (690)\ttotal: 2m 2s\tremaining: 1m 44s\n",
      "800:\tlearn: 0.8169065\ttest: 0.7479633\tbest: 0.7495970 (785)\ttotal: 2m 20s\tremaining: 1m 27s\n",
      "900:\tlearn: 0.8231049\ttest: 0.7506429\tbest: 0.7513221 (896)\ttotal: 2m 37s\tremaining: 1m 9s\n",
      "1000:\tlearn: 0.8287445\ttest: 0.7513356\tbest: 0.7523116 (972)\ttotal: 2m 54s\tremaining: 52s\n",
      "1100:\tlearn: 0.8338657\ttest: 0.7523822\tbest: 0.7546881 (1042)\ttotal: 3m 12s\tremaining: 34.8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7546880922\n",
      "bestIteration = 1042\n",
      "\n",
      "Shrink model to first 1043 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4777310\ttest: 0.4629500\tbest: 0.4629500 (0)\ttotal: 59.6ms\tremaining: 1m 41s\n",
      "100:\tlearn: 0.6914280\ttest: 0.6697465\tbest: 0.6726950 (98)\ttotal: 6.24s\tremaining: 1m 38s\n",
      "200:\tlearn: 0.7335890\ttest: 0.7065198\tbest: 0.7078501 (176)\ttotal: 12.1s\tremaining: 1m 30s\n",
      "300:\tlearn: 0.7599923\ttest: 0.7296911\tbest: 0.7296911 (300)\ttotal: 17.8s\tremaining: 1m 22s\n",
      "400:\tlearn: 0.7760616\ttest: 0.7242300\tbest: 0.7306957 (303)\ttotal: 23.8s\tremaining: 1m 17s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7306956963\n",
      "bestIteration = 303\n",
      "\n",
      "Shrink model to first 304 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4703165\ttest: 0.4610996\tbest: 0.4610996 (0)\ttotal: 44.3ms\tremaining: 1m 24s\n",
      "100:\tlearn: 0.5955340\ttest: 0.5871799\tbest: 0.5876816 (96)\ttotal: 4.8s\tremaining: 1m 25s\n",
      "200:\tlearn: 0.6419540\ttest: 0.6437445\tbest: 0.6437445 (199)\ttotal: 10.1s\tremaining: 1m 25s\n",
      "300:\tlearn: 0.6623049\ttest: 0.6517967\tbest: 0.6519246 (296)\ttotal: 15.3s\tremaining: 1m 21s\n",
      "400:\tlearn: 0.6823839\ttest: 0.6735754\tbest: 0.6742330 (399)\ttotal: 20.1s\tremaining: 1m 15s\n",
      "500:\tlearn: 0.6930340\ttest: 0.6856851\tbest: 0.6861582 (492)\ttotal: 24.9s\tremaining: 1m 9s\n",
      "600:\tlearn: 0.7024612\ttest: 0.6931236\tbest: 0.6958597 (586)\ttotal: 29.8s\tremaining: 1m 4s\n",
      "700:\tlearn: 0.7098353\ttest: 0.6993683\tbest: 0.6995721 (695)\ttotal: 34.9s\tremaining: 59.7s\n",
      "800:\tlearn: 0.7158764\ttest: 0.7046891\tbest: 0.7046891 (797)\ttotal: 39.7s\tremaining: 54.4s\n",
      "900:\tlearn: 0.7267800\ttest: 0.7074465\tbest: 0.7079127 (866)\ttotal: 44.5s\tremaining: 49.4s\n",
      "1000:\tlearn: 0.7329412\ttest: 0.7103656\tbest: 0.7111856 (964)\ttotal: 49.2s\tremaining: 44.2s\n",
      "1100:\tlearn: 0.7390775\ttest: 0.7153374\tbest: 0.7155848 (1096)\ttotal: 54.9s\tremaining: 39.8s\n",
      "1200:\tlearn: 0.7435539\ttest: 0.7159521\tbest: 0.7176074 (1188)\ttotal: 1m\tremaining: 35s\n",
      "1300:\tlearn: 0.7494826\ttest: 0.7182029\tbest: 0.7182029 (1282)\ttotal: 1m 4s\tremaining: 29.9s\n",
      "1400:\tlearn: 0.7545448\ttest: 0.7198325\tbest: 0.7211624 (1378)\ttotal: 1m 9s\tremaining: 24.9s\n",
      "1500:\tlearn: 0.7577513\ttest: 0.7230774\tbest: 0.7230774 (1497)\ttotal: 1m 14s\tremaining: 19.8s\n",
      "1600:\tlearn: 0.7625800\ttest: 0.7249306\tbest: 0.7249306 (1574)\ttotal: 1m 19s\tremaining: 14.8s\n",
      "1700:\tlearn: 0.7646601\ttest: 0.7268274\tbest: 0.7268274 (1700)\ttotal: 1m 24s\tremaining: 9.88s\n",
      "1800:\tlearn: 0.7673600\ttest: 0.7287435\tbest: 0.7299326 (1731)\ttotal: 1m 29s\tremaining: 4.92s\n",
      "1899:\tlearn: 0.7701172\ttest: 0.7328968\tbest: 0.7332222 (1892)\ttotal: 1m 34s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7332221824\n",
      "bestIteration = 1892\n",
      "\n",
      "Shrink model to first 1893 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4919946\ttest: 0.4759614\tbest: 0.4759614 (0)\ttotal: 125ms\tremaining: 1m 2s\n",
      "100:\tlearn: 0.7064500\ttest: 0.6923211\tbest: 0.6933821 (98)\ttotal: 12s\tremaining: 47.5s\n",
      "200:\tlearn: 0.7529236\ttest: 0.7143593\tbest: 0.7143593 (200)\ttotal: 23.4s\tremaining: 34.8s\n",
      "300:\tlearn: 0.7755691\ttest: 0.7344126\tbest: 0.7347891 (277)\ttotal: 35s\tremaining: 23.2s\n",
      "400:\tlearn: 0.7887572\ttest: 0.7435863\tbest: 0.7435863 (399)\ttotal: 46.5s\tremaining: 11.5s\n",
      "499:\tlearn: 0.7995589\ttest: 0.7421718\tbest: 0.7488047 (486)\ttotal: 57.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7488047402\n",
      "bestIteration = 486\n",
      "\n",
      "Shrink model to first 487 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4847216\ttest: 0.4682424\tbest: 0.4682424 (0)\ttotal: 72.9ms\tremaining: 51s\n",
      "100:\tlearn: 0.6629730\ttest: 0.6566255\tbest: 0.6566255 (100)\ttotal: 9.48s\tremaining: 56.2s\n",
      "200:\tlearn: 0.7037670\ttest: 0.6875957\tbest: 0.6882534 (198)\ttotal: 18.7s\tremaining: 46.5s\n",
      "300:\tlearn: 0.7281873\ttest: 0.7054898\tbest: 0.7058931 (290)\ttotal: 27.9s\tremaining: 37s\n",
      "400:\tlearn: 0.7478171\ttest: 0.7141805\tbest: 0.7148735 (370)\ttotal: 37.2s\tremaining: 27.8s\n",
      "500:\tlearn: 0.7603097\ttest: 0.7210600\tbest: 0.7213853 (497)\ttotal: 46.4s\tremaining: 18.4s\n",
      "600:\tlearn: 0.7704588\ttest: 0.7295289\tbest: 0.7295289 (581)\ttotal: 55.9s\tremaining: 9.21s\n",
      "699:\tlearn: 0.7793817\ttest: 0.7325067\tbest: 0.7328035 (664)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7328034644\n",
      "bestIteration = 664\n",
      "\n",
      "Shrink model to first 665 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4995109\ttest: 0.4926757\tbest: 0.4926757 (0)\ttotal: 680ms\tremaining: 16m 59s\n",
      "100:\tlearn: 0.7375279\ttest: 0.7077130\tbest: 0.7077130 (100)\ttotal: 37.5s\tremaining: 8m 40s\n",
      "200:\tlearn: 0.7802135\ttest: 0.7348476\tbest: 0.7350036 (195)\ttotal: 1m 13s\tremaining: 7m 52s\n",
      "300:\tlearn: 0.8015806\ttest: 0.7435920\tbest: 0.7445966 (299)\ttotal: 1m 48s\tremaining: 7m 12s\n",
      "400:\tlearn: 0.8141132\ttest: 0.7474748\tbest: 0.7490935 (382)\ttotal: 2m 24s\tremaining: 6m 36s\n",
      "500:\tlearn: 0.8243012\ttest: 0.7514291\tbest: 0.7521858 (431)\ttotal: 2m 59s\tremaining: 5m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7521858001\n",
      "bestIteration = 431\n",
      "\n",
      "Shrink model to first 432 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alrxp\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [500, 2000] and step=200, but the range is not divisible by `step`. It will be replaced by [500, 1900].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5012050\ttest: 0.4866801\tbest: 0.4866801 (0)\ttotal: 338ms\tremaining: 7m 18s\n",
      "100:\tlearn: 0.7091296\ttest: 0.6972405\tbest: 0.6995819 (97)\ttotal: 32.4s\tremaining: 6m 25s\n",
      "200:\tlearn: 0.7632866\ttest: 0.7284256\tbest: 0.7284256 (200)\ttotal: 1m 4s\tremaining: 5m 51s\n",
      "300:\tlearn: 0.7883671\ttest: 0.7392175\tbest: 0.7399531 (292)\ttotal: 1m 36s\tremaining: 5m 18s\n",
      "400:\tlearn: 0.8086442\ttest: 0.7463711\tbest: 0.7463711 (400)\ttotal: 2m 7s\tremaining: 4m 45s\n",
      "500:\tlearn: 0.8213135\ttest: 0.7502744\tbest: 0.7525873 (479)\ttotal: 2m 39s\tremaining: 4m 13s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7525873244\n",
      "bestIteration = 479\n",
      "\n",
      "Shrink model to first 480 iterations.\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_catboost.optimize(objective_catboost, n_trials=10,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7983e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 500,\n",
       " 'depth': 5,\n",
       " 'learning_rate': 0.12,\n",
       " 'l2_leaf_reg': 0.16960494498640707,\n",
       " 'colsample_bylevel': 0.23613115193045567}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_catboost.best_params #–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e07c8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cb_model = catboost.CatBoostClassifier(**study_catboost.best_params) #–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "293dba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.6741021\ttest: 1.6680147\tbest: 1.6680147 (0)\ttotal: 120ms\tremaining: 59.7s\n",
      "100:\tlearn: 0.9673294\ttest: 0.9834026\tbest: 0.9832746 (99)\ttotal: 17.7s\tremaining: 1m 10s\n",
      "200:\tlearn: 0.8446145\ttest: 0.9222759\tbest: 0.9222355 (199)\ttotal: 35.6s\tremaining: 52.9s\n",
      "300:\tlearn: 0.7704406\ttest: 0.8988053\tbest: 0.8985376 (299)\ttotal: 54.3s\tremaining: 35.9s\n",
      "400:\tlearn: 0.7148313\ttest: 0.8798525\tbest: 0.8798525 (400)\ttotal: 1m 13s\tremaining: 18.1s\n",
      "499:\tlearn: 0.6717635\ttest: 0.8729500\tbest: 0.8717626 (475)\ttotal: 1m 32s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8717625816\n",
      "bestIteration = 475\n",
      "\n",
      "Shrink model to first 476 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2b1c1c3b0d0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cb_model.fit(train_pool,\n",
    "                  eval_set = test_pool,\n",
    "                  verbose=100,\n",
    "                  early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "89da17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = best_cb_model.predict(valid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ab0f4760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6189451022604952"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid['SentimentScore'].values, best_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bdc02",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ ruBERT-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "bf27a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'very_negative': 0,\n",
    "    'negative': 1,\n",
    "    'neutral': 2,\n",
    "    'positive': 3,\n",
    "    'very_positive': 4\n",
    "}\n",
    "\n",
    "id2label = {\n",
    "    0: 'very_negative',\n",
    "    1: 'negative',\n",
    "    2: 'neutral',\n",
    "    3: 'positive',\n",
    "    4: 'very_positive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3209a-9f95-42c6-9f48-4f524f49e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_texts.SentimentScore -= 1 #–í—ã—á–∏—Ç–∞–µ–º –µ–¥–∏–Ω–∏—Ü—É, —Ç.–∫. –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è rubert-tiny - 0, –∞ –Ω–µ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9e233035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 5, 3, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_texts.SentimentScore.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "fba86670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny\", num_labels=len(id2label.keys()), label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b15eb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_of_dicts = [] #–ø—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫ –≤–∏–¥—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "for idx, row in sentiment_texts.iterrows():\n",
    "    text = row['MessageText']\n",
    "    label = row['SentimentScore']\n",
    "    data_list_of_dicts.append({'text': text, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "f777b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "def tokenize_data(text):\n",
    "    return tokenizer(text['text'], padding=True, truncation=True, max_length=256, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "9718e2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffle(data_list_of_dicts)\n",
    "train_bert = data_list_of_dicts[:7300]\n",
    "test_bert = data_list_of_dicts[7300:]\n",
    "train_bert = Dataset.from_pandas(pd.DataFrame(data=train_bert))\n",
    "test_bert = Dataset.from_pandas(pd.DataFrame(data=test_bert))\n",
    "tokenized_train = train_bert.map(tokenize_data, batched=True)\n",
    "tokenized_test = test_bert.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "d3a7d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred): #—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {'f1': f1_score, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "e7455547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"akra_model\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "e91a76ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/805 00:49 < 1:34:30, 0.14 it/s, Epoch 0.07/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[577], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2750\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2753\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2775\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2774\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2775\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2777\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:463\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "a3f0986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.829366  , -0.11736619,  0.95662   ,  0.9494317 ,  0.27798185]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(single_test['input_ids'], single_test['token_type_ids'], single_test['attention_mask']).logits.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c49835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
