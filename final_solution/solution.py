import typing as tp
import pandas as pd
import pickle
import re
from nltk import word_tokenize
from natasha import (
    Segmenter,
    NewsNERTagger,
    ORG,
    Doc,
    NewsEmbedding,
)
import sklearn
from sklearn.metrics.pairwise import cosine_similarity
from joblib import load
import numpy as np

emb = NewsEmbedding()
segmenter = Segmenter()
ner_tagger = NewsNERTagger(emb)

EntityScoreType = tp.Tuple[int, float]  # (entity_id, entity_score)
MessageResultType = tp.List[
    EntityScoreType
]  # list of entity scores,
#    for example, [(entity_id, entity_score) for entity_id, entity_score in entities_found]

def score_texts(
    messages: tp.Iterable[str], *args, **kwargs
) -> tp.Iterable[MessageResultType]:
    most_common_tokens = [
        '–º–ª—Ä–¥',
        '–º–ª–Ω',
        '—Ç—Ä–ª–Ω',
        '—Ä—É–±',
        '–≥–æ–¥–∞',
        '–∫–æ–º–ø–∞–Ω–∏–∏',
        '–∫–æ–º–ø–∞–Ω–∏—è',
        '–∫–æ–º–ø–∞–Ω–∏–π',
        '–∞–∫—Ü–∏–∏',
        '–∞–∫—Ü–∏–π',
        '—Ä—É–±–ª–µ–π',
        '–≥–æ–¥',
        '–Ω–∞',
        '–≥–æ–¥—É',
        '–ø–æ',
        '—Å—à–∞',
        '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã',
        '–º—Å—Ñ–æ',
        '–Ω–µ—Ñ—Ç—å',
        '–Ω–µ—Ñ—Ç–∏',
        'ru',
        '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤',
        '–î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤'
        '–ø—Ä–∏',
        '—Å–∏—Å—Ç–µ–º–∞',
        '30–º—Å–∫',
        '00–º—Å–∫',
        '—Å–æ–≤–µ—Ç',
        '–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤',
         'ak47pfl',
        '–µ—Å–ª–∏',
        '–≤–æ–ø—Ä–æc',
        '–Ω–æ–≤–æ—Å—Ç–∏',
        '–ø–æ–¥—Ä–æ–±–Ω–µ–µ',
        '—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö',
        '—Ä–∞–º–∫–∞—Ö',
        'ipo',
        '–≤–∑–≥–ª—è–¥',
        '—Ä—Ñ',
        '—Ä–æ—Å—Å–∏–∏',
        '—Ä–æ—Å—Å–∏—è',
        '—Ä–æ—Å—Å–∏–µ',
        '–Ω–µ',
        '–¥–Ω—è',
        '–¥–∏–≤–∏–¥–µ–Ω–¥—ã',
        '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
        '–æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å',
        '–±–∏—Ä–∂–∞',
        '–±–∏—Ä–∂–µ',
        '–±–∏—Ä–∂–∏',
        '–∫–≤',
        '–º—Å–∫',
        '–≥–≥',
        '—Ñ–Ω',
        'upd'
    ]

    not_company_name_natasha = [
        '–ª–æ–Ω–≥',
        '–º–∏—Ä–∞',
        '–≥–æ—Ä—è—á—É—é',
        '–∫–∞–∑–Ω–∞—á–µ–π—Å—Ç–≤–æ',
        '—Ä–∞—Å–ø–∞–¥—Å–∫–æ–π',
        '–¥–µ—Ç—Å–∫–æ–≥–æ',
        '–≤–µ–¥–æ–º–æ—Å—Ç–∏',
        '–∞–∫—Ü–∏–æ–Ω–µ—Ä—ã',
        '—à–æ—Ä—Ç',
        '–¥–µ—Å—è—Ç–∫—É',
        '–∞–∫—Ü–∏—è'
        '—Ü–±',
        '–º–æ—Å–±–∏—Ä–∂–∏',
        '–º–æ—Å–±–∏—Ä–∂–∞',
        '–±–∞–Ω–∫–∞',
        '—Å–¥',
        '–∫–æ–º–ø–∞–Ω–∏—è',
        '—Å–æ–≤–µ—Ç',
        '—Å–∏—Å—Ç–µ–º–∞',
        '–ø—Ä–∞–π–º',
        '–∞–æ',
        '—Ä–∞–æ',
        '–º–æ—Å–∫–æ–≤—Å–∫–æ–π',
        '—Ç–∞—Å—Å',
        '—Ñ—Ä—Å',
        '–º–∏–Ω—Ñ–∏–Ω',
        '–≥—Ä—É–ø–ø—ã',
        '–±–∏—Ä–∂–∞',
        '–±–∏—Ä–∂–µ',
        '–±–∏—Ä–∂–∏',
        'en',
        '—Ñ–∞—Å',
        '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å',
        '–∑–∞',
        '–≤—ã—Ä—É—á–∫–∞',
        '–Ω–µ',
        '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π',
        '—Å–º–∏',
        '—Ñ–ª—ç—Ç',
        '—Ñ–æ–Ω–¥',
        '—Ç–æ–ø',
        '–Ω–æ–≤–æ—Å—Ç–∏',
        '–±–æ–∫–æ–≤–∏–∫',
        '–º–∏–Ω—Ñ–∏–Ω–∞',
        '—Å–∏—Å—Ç–µ–º—ã',
        '—Ü–µ–Ω—Ç—Ä',
        '—Ñ–Ω',
        '—Å–µ–≥–æ–¥–Ω—è',
        '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª',
        '–ø—Ä–æ–≥–Ω–æ–∑',
        '—Å–∏–≥–Ω–∞–ª–æ–≤',
        '–æ—Ç',
        '—Ä–∞–∑–º–µ—Ä–µ',
        '—Ä–∏–∞',
        '–æ–±—ä–µ–º',
        '–∞—É—Ç—Å–∞–π–¥–µ—Ä—ã',
        '–æ—Ç—á–µ—Ç',
        '–ø—Ä–∞–≤–ª–µ–Ω–∏—è',
        '–º–æ—Å–∫–æ–≤—Å–∫–∏–π',
        '—Å–ª—É–∂–±–∞',
        '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π',
        '–ø—Ä–∏–∫–∞–∑',
        '–∏–Ω—Ç–µ—Ä—Ñ–∞–∫—Å–∞',
        '—Ä–∞—Å–ø–∞–¥—Å–∫–æ–π',
        '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π',
        '–¥–æ'
    ]

    russian_stopwords = ['–∏',
     '–≤',
     '–≤–æ',
     '–Ω–µ',
     '—á—Ç–æ',
     '–æ–Ω',
     '–Ω–∞',
     '—è',
     '—Å',
     '—Å–æ',
     '–∫–∞–∫',
     '–∞',
     '—Ç–æ',
     '–≤—Å–µ',
     '–æ–Ω–∞',
     '—Ç–∞–∫',
     '–µ–≥–æ',
     '–Ω–æ',
     '–¥–∞',
     '—Ç—ã',
     '–∫',
     '—É',
     '–∂–µ',
     '–≤—ã',
     '–∑–∞',
     '–±—ã',
     '–ø–æ',
     '—Ç–æ–ª—å–∫–æ',
     '–µ–µ',
     '–º–Ω–µ',
     '–±—ã–ª–æ',
     '–≤–æ—Ç',
     '–æ—Ç',
     '–º–µ–Ω—è',
     '–µ—â–µ',
     '–Ω–µ—Ç',
     '–æ',
     '–∏–∑',
     '–µ–º—É',
     '—Ç–µ–ø–µ—Ä—å',
     '–∫–æ–≥–¥–∞',
     '–¥–∞–∂–µ',
     '–Ω—É',
     '–≤–¥—Ä—É–≥',
     '–ª–∏',
     '–µ—Å–ª–∏',
     '—É–∂–µ',
     '–∏–ª–∏',
     '–Ω–∏',
     '–±—ã—Ç—å',
     '–±—ã–ª',
     '–Ω–µ–≥–æ',
     '–¥–æ',
     '–≤–∞—Å',
     '–Ω–∏–±—É–¥—å',
     '–æ–ø—è—Ç—å',
     '—É–∂',
     '–≤–∞–º',
     '–≤–µ–¥—å',
     '—Ç–∞–º',
     '–ø–æ—Ç–æ–º',
     '—Å–µ–±—è',
     '–Ω–∏—á–µ–≥–æ',
     '–µ–π',
     '–º–æ–∂–µ—Ç',
     '–æ–Ω–∏',
     '—Ç—É—Ç',
     '–≥–¥–µ',
     '–µ—Å—Ç—å',
     '–Ω–∞–¥–æ',
     '–Ω–µ–π',
     '–¥–ª—è',
     '–º—ã',
     '—Ç–µ–±—è',
     '–∏—Ö',
     '—á–µ–º',
     '–±—ã–ª–∞',
     '—Å–∞–º',
     '—á—Ç–æ–±',
     '–±–µ–∑',
     '–±—É–¥—Ç–æ',
     '—á–µ–≥–æ',
     '—Ä–∞–∑',
     '—Ç–æ–∂–µ',
     '—Å–µ–±–µ',
     '–ø–æ–¥',
     '–±—É–¥–µ—Ç',
     '–∂',
     '—Ç–æ–≥–¥–∞',
     '–∫—Ç–æ',
     '—ç—Ç–æ—Ç',
     '—Ç–æ–≥–æ',
     '–ø–æ—Ç–æ–º—É',
     '—ç—Ç–æ–≥–æ',
     '–∫–∞–∫–æ–π',
     '—Å–æ–≤—Å–µ–º',
     '–Ω–∏–º',
     '–∑–¥–µ—Å—å',
     '—ç—Ç–æ–º',
     '–æ–¥–∏–Ω',
     '–ø–æ—á—Ç–∏',
     '–º–æ–π',
     '—Ç–µ–º',
     '—á—Ç–æ–±—ã',
     '–Ω–µ–µ',
     '—Å–µ–π—á–∞—Å',
     '–±—ã–ª–∏',
     '–∫—É–¥–∞',
     '–∑–∞—á–µ–º',
     '–≤—Å–µ—Ö',
     '–Ω–∏–∫–æ–≥–¥–∞',
     '–º–æ–∂–Ω–æ',
     '–ø—Ä–∏',
     '–Ω–∞–∫–æ–Ω–µ—Ü',
     '–¥–≤–∞',
     '–æ–±',
     '–¥—Ä—É–≥–æ–π',
     '—Ö–æ—Ç—å',
     '–ø–æ—Å–ª–µ',
     '–Ω–∞–¥',
     '–±–æ–ª—å—à–µ',
     '—Ç–æ—Ç',
     '—á–µ—Ä–µ–∑',
     '—ç—Ç–∏',
     '–Ω–∞—Å',
     '–ø—Ä–æ',
     '–≤—Å–µ–≥–æ',
     '–Ω–∏—Ö',
     '–∫–∞–∫–∞—è',
     '–º–Ω–æ–≥–æ',
     '—Ä–∞–∑–≤–µ',
     '—Ç—Ä–∏',
     '—ç—Ç—É',
     '–º–æ—è',
     '–≤–ø—Ä–æ—á–µ–º',
     '—Ö–æ—Ä–æ—à–æ',
     '—Å–≤–æ—é',
     '—ç—Ç–æ–π',
     '–ø–µ—Ä–µ–¥',
     '–∏–Ω–æ–≥–¥–∞',
     '–ª—É—á—à–µ',
     '—á—É—Ç—å',
     '—Ç–æ–º',
     '–Ω–µ–ª—å–∑—è',
     '—Ç–∞–∫–æ–π',
     '–∏–º',
     '–±–æ–ª–µ–µ',
     '–≤—Å–µ–≥–¥–∞',
     '–∫–æ–Ω–µ—á–Ω–æ',
     '–≤—Å—é',
     '–º–µ–∂–¥—É',
     '—ç—Ç–æ',
     '–∫–∞–∫',
     '—Ç–∞–∫',
     '–∏',
     '–≤',
     '–Ω–∞–¥',
     '–∫',
     '–¥–æ',
     '–Ω–µ',
     '–Ω–∞',
     '–Ω–æ',
     '–∑–∞',
     '—Ç–æ',
     '—Å',
     '–ª–∏',
     '–∞',
     '–≤–æ',
     '–æ—Ç',
     '—Å–æ',
     '–¥–ª—è',
     '–æ',
     '–∂–µ',
     '–Ω—É',
     '–≤—ã',
     '–±—ã',
     '—á—Ç–æ',
     '–∫—Ç–æ',
     '–æ–Ω',
     '–æ–Ω–∞',
     '–æ–Ω–æ',
     '–∏–∑-–∑–∞',
     '—Ç–∞–∫–∂–µ'
     ]

    with open('latin_companies_mentions.pickle', 'rb') as f:
        latin_comps_mentions = pickle.load(f)

    with open('all_company_names_dict.pickle', 'rb') as f:
        all_names_dict = pickle.load(f)

    model_tfidf = load('TFIDF_fitted_model.joblib')


    df = pd.DataFrame(messages, columns=['MessageText'])

    def message_text_preprocess(text):  # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        remove_punctuation = '!"#$%&\'*+,./:;<=>?@[\\]^_`{|}~``üá∑üá∫'')('
        text = re.sub(r'\s+', ' ',
                      text)  # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
        text = re.sub(r'(?<=[^\w\d])-|-(?=[^\w\d])|[^\w\d\s-]', '',
                      text)  # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥–µ—Ñ–∏—Å–æ–≤
        text = re.sub(r'\+\d{1,2}\s\(\d{3}\)\s\d{3}-\d{2}-\d{2}', '', text)  # —É–¥–∞–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        text = re.sub(r'https?://\S+', '', text)  # —É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'\s+', ' ', text)  # –µ—â–µ —Ä–∞–∑ —É–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã, –µ—Å–ª–∏ —Ç–∞–∫–æ–≤—ã–µ –æ—Å—Ç–∞–ª–∏—Å—å
        text = re.sub('‚Ä¢', '', text)
        text = re.sub("''", '', text)
        text = re.sub(r'[¬´¬ª]', '', text)
        text = re.sub(r'\d+', '', text)  # —É–¥–∞–ª—è–µ–º –≤—Å–µ —Ü–∏—Ñ—Ä—ã
        text = re.sub(r'\b\w\b', '', text)  # —É–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã

        tokens = word_tokenize(text)  # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        filtered_tokens = [token for token in tokens if
                           token not in remove_punctuation and token not in russian_stopwords and token.lower() not in latin_comps_mentions and token.lower() not in most_common_tokens]

        return " ".join(filtered_tokens)

    df['MessageText'] = df['MessageText'].apply(message_text_preprocess)

    def fill_natasha_mentions(text):
        doc = Doc(text)
        doc.segment(segmenter)
        doc.tag_ner(ner_tagger)
        orgs = []
        for item in doc.spans:
            if len(orgs) >= 3:  # –ï—Å–ª–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–æ 3 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
                break
            if item.type == 'ORG' and item.text.lower() not in not_company_name_natasha:
                orgs.append(item.text[:10])
        return orgs

    df['natasha_mentions'] = df['MessageText'].apply(fill_natasha_mentions)

    def find_best_companies(model_tfidf, mentions,
                            all_names_dict):  # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
        if len(mentions) == 0:  # –¥–ª—è —Å–ª—É—á–∞—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ–∫—Å—Ç–µ
            return 0
        natasha_vector = model_tfidf.transform(
            [' '.join(mentions)])  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è Natasha –∏ –∫–æ–º–ø–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–∞ TF-IDF
        company_vectors = model_tfidf.transform([' '.join(company_names) for company_names in all_names_dict.values()])

        similarities = cosine_similarity(natasha_vector,
                                         company_vectors)  # –ù–∞—Ö–æ–¥–∏–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ –∏ –∫–æ–º–ø–∞–Ω–∏—è–º–∏

        best_company_id = similarities.argsort(axis=1)[:, -1] + 1  # –∫–æ–º–ø–∞–Ω–∏—è —Å –Ω–∞–∏–≤—ã—Å—à–∏–º —Å–∫–æ—Ä–æ–º
        second_best_company_id = similarities.argsort(axis=1)[:, -2] + 1  # –∫–æ–º–ø–∞–Ω–∏—è —Å–æ –≤—Ç–æ—Ä—ã–º –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ —Å–∫–æ—Ä–æ–º

        if similarities.max(axis=1)[0] - np.partition(similarities, -2, axis=1)[:, -2][
            0] > 0.05:  # —É—Å–ª–æ–≤–∏–µ —Ä–∞–∑–Ω–∏—Ü—ã —Ç–æ–ø-1 –∏ —Ç–æ–ø-2 —Å–∫–æ—Ä–∞
            return best_company_id[0]
        else:
            return [best_company_id[0], second_best_company_id[0]]

    df['predict_id'] = df['natasha_mentions'].apply(
        lambda x: find_best_companies(model_tfidf, x, all_names_dict))

    df = df.explode('predict_id')

    to_predict_tfidf = model_tfidf.transform(df['MessageText'].values)

    clf = load('RF_clf.joblib')

    df['predict_sentiment'] = clf.predict(to_predict_tfidf)

    grouped = df.groupby('MessageText')

    def transform_group(group): #–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        pairs = []
        for _, row in group.iterrows():
            if row['predict_id'] == 0:
                pairs.append(())
            else:
                pairs.append((row['predict_id'], row['predict_sentiment']))
        return [pairs]

    result = [transform_group(group) for _, group in grouped] #–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–ø–∏—Å–æ–∫

    return result


    """
    Main function (see tests for more clarifications)
    Args:
        messages (tp.Iterable[str]): any iterable of strings (utf-8 encoded text messages)

    Returns:
        tp.Iterable[tp.Tuple[int, float]]: for any messages returns MessageResultType object
    -------
    Clarifications:
    >>> assert all([len(m) < 10 ** 11 for m in messages]) # all messages are shorter than 2048 characters
    """
    raise NotImplementedError
